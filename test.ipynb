{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a highly precise information extractor designed to identify key research entities within an essay abstract.  You are very strict; you will absolutely not associate or produce information that is not in the original text. Adhere strictly to the following guidelines:\\n\\n# Core Principles:\\n\\n* Focus Exclusively on the Abstract: Base your analysis solely on the provided abstract text.\\n\\n* Identify Research Entities: Pinpoint the distinct concepts, methods, variables, or objects that are central to the research investigation.\\n\\n* Eliminate Duplicates: Ensure each identified entity is unique within the output.\\n\\n* Resolve Ambiguity: If an entity\\'s meaning is unclear, deduce its most likely interpretation based on the context.\\n\\n* Simplify Complex Terminology:  If technical jargon is present, strive to express the entity in more accessible language.\\n\\n* Highlight Novel Paradigms: If the abstract proposes a new model, framework, or approach, explicitly include it in the output.\\n\\n# **Instructions:**\\n\\n1. **Prioritize Key Entities:** Focus on extracting the most salient and relevant entities that contribute to the core research focus.\\n2. **Limit to 8 Entities (Maximum):** Extract a maximum of 8 entities, consolidating closely related concepts where appropriate.\\n3. **Provide JSON Output Only:** Present your response exclusively in the specified JSON format, with no additional commentary.\\n\\n# Instructions:\\n\\n- Focus on the Abstract: Base your extraction solely on the provided abstract.\\n- Identify Research Entities: Pinpoint all the distinct concepts, methods, or objects central to the research.\\n- Eliminate Duplicates: Ensure that each entity appears only once in the output JSON.\\n- Handle Ambiguity: If the entity is unclear, try to identify the most likely candidate based on the context.\\n- Simplify Complex Language: If the language is highly technical, attempt to express the entity in more general terms.\\n- if the abstract propose a new paradigm, you must extract it.\\n- Output in JSON: Provide your answer exclusively in the specified JSON format.\\n- You should only give the JSON output and nothing else.\\n- You should extract at most 8 entities and combine those entities are close to each other\\n\\n# example\\n\\n## example abstract\\n\\nLarge Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for in-formation extraction (IE) tasks, however, re-mains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. There-fore, we conclude that LLMs are not effec-tive few-shot information extractors in general 1. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers.\\nBy prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our pre-liminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.\\n\\n##  example output\\n\\n{\"entity1\": \"Large Language Models (LLMs)\", \"entity2\": \"information extraction (IE)\", \"entity3\": \"fine-tuned SLMs\", \"entity4\": \"prompting strategies\", \"entity5\": \"adaptive filter-then-rerank paradigm\"}\\n\\nNow, replace the content after the colon with the unique research entities of the following essay abstract:'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file='prompt.md', mode='r', encoding='utf-8') as file:\n",
    "    prompt = file.read()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import openai\n",
    "def extract_entity(abstract:str):\n",
    "\n",
    "#     openai.api_key = \"YOUR_API_KEY\" # openai version\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-40\", \n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": prompt + abstract}\n",
    "#     ],\n",
    "#     temperature=0  \n",
    "# )\n",
    "\n",
    "#     try:\n",
    "#         return json.loads(response.choices[0].message.content)\n",
    "#     except json.JSONDecodeError: \n",
    "#         return \"Not JSON format\"\n",
    "\n",
    "    response = ollama.chat(model='llama3.1', options={\"temperature\":0}, messages=[{\n",
    "        'role':'user',\n",
    "        'content': prompt + abstract\n",
    "    }])\n",
    "    \n",
    "    try: return(json.loads(response['message']['content']))\n",
    "\n",
    "    except json.JSONDecodeError: \n",
    "        return \"Not JSON format\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'decomposition-based evolutionary multi-objective optimization',\n",
       " 'entity2': 'MOEA/D',\n",
       " 'entity3': 'multi-objective evolutionary algorithms',\n",
       " 'entity4': 'data mining approaches',\n",
       " 'entity5': 'knowledge graph',\n",
       " 'entity6': 'topic modeling techniques',\n",
       " 'entity7': 'citation networks',\n",
       " 'entity8': 'decomposition'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_abstract = 'Abstract:This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature re-lated to multi-objective evolutionary algorithms based on decomposition (MOEA/D). Complementary to the first part, here we employ a series of advanced data mining approaches to provide a compre-hensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol. In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5, 400 papers, 10, 000 authors, 400 venues, and 1, 600 institutions for MOEA/D research. We start our analysis with basic descriptive statistics. Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships. We also explored the col-laboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers. Our data mining results here, combined with the ex-pert review in Part I1, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective. Keywords: Multi-objective optimization, decomposition, data mining, topic modeling, network analysis, data visualization.'\n",
    "extract_entity(abstract=test_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('source_document.csv')\n",
    "test = test[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>End-to-end attention-based large vocabulary sp...</td>\n",
       "      <td>Many state-of-the-art Large Vocabulary Continu...</td>\n",
       "      <td>{'entity1': 'Large Vocabulary Continuous Speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "      <td>{'entity1': 'deep contextualized word represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>End-to-end memory networks</td>\n",
       "      <td>We introduce a neural network with a recurrent...</td>\n",
       "      <td>{'entity1': 'neural network', 'entity2': 'recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepTox: Toxicity prediction using deep learning</td>\n",
       "      <td>The Tox21 Data Challenge has been the largest ...</td>\n",
       "      <td>{'entity1': 'Tox21 Data Challenge', 'entity2':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language models as knowledge bases?</td>\n",
       "      <td>Recent progress in pretraining language models...</td>\n",
       "      <td>{'entity1': 'pretraining language models', 'en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A state-of-the-art survey on deep learning the...</td>\n",
       "      <td>In recent years, deep learning has garnered tr...</td>\n",
       "      <td>{'entity1': 'Deep Learning', 'entity2': 'Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>“So what if ChatGPT wrote it?” Multidisciplina...</td>\n",
       "      <td>Transformative artificially intelligent tools,...</td>\n",
       "      <td>{'entity1': 'ChatGPT', 'entity2': 'generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Sequence-level knowledge distillation</td>\n",
       "      <td>Neural machine translation (NMT) offers a nove...</td>\n",
       "      <td>{'entity1': 'Neural machine translation (NMT)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Transformers in Vision: A Survey</td>\n",
       "      <td>Astounding results from Transformer models on ...</td>\n",
       "      <td>{'entity1': 'Transformer models', 'entity2': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learned in translation: Contextualized word ve...</td>\n",
       "      <td>Computer vision has benefited from initializin...</td>\n",
       "      <td>{'entity1': 'deep LSTM encoder', 'entity2': 'a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   End-to-end attention-based large vocabulary sp...   \n",
       "1            Deep contextualized word representations   \n",
       "2                          End-to-end memory networks   \n",
       "3    DeepTox: Toxicity prediction using deep learning   \n",
       "4                 Language models as knowledge bases?   \n",
       "..                                                ...   \n",
       "95  A state-of-the-art survey on deep learning the...   \n",
       "96  “So what if ChatGPT wrote it?” Multidisciplina...   \n",
       "97              Sequence-level knowledge distillation   \n",
       "98                   Transformers in Vision: A Survey   \n",
       "99  Learned in translation: Contextualized word ve...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Many state-of-the-art Large Vocabulary Continu...   \n",
       "1   We introduce a new type of deep contextualized...   \n",
       "2   We introduce a neural network with a recurrent...   \n",
       "3   The Tox21 Data Challenge has been the largest ...   \n",
       "4   Recent progress in pretraining language models...   \n",
       "..                                                ...   \n",
       "95  In recent years, deep learning has garnered tr...   \n",
       "96  Transformative artificially intelligent tools,...   \n",
       "97  Neural machine translation (NMT) offers a nove...   \n",
       "98  Astounding results from Transformer models on ...   \n",
       "99  Computer vision has benefited from initializin...   \n",
       "\n",
       "                                               entity  \n",
       "0   {'entity1': 'Large Vocabulary Continuous Speec...  \n",
       "1   {'entity1': 'deep contextualized word represen...  \n",
       "2   {'entity1': 'neural network', 'entity2': 'recu...  \n",
       "3   {'entity1': 'Tox21 Data Challenge', 'entity2':...  \n",
       "4   {'entity1': 'pretraining language models', 'en...  \n",
       "..                                                ...  \n",
       "95  {'entity1': 'Deep Learning', 'entity2': 'Machi...  \n",
       "96  {'entity1': 'ChatGPT', 'entity2': 'generative ...  \n",
       "97  {'entity1': 'Neural machine translation (NMT)'...  \n",
       "98  {'entity1': 'Transformer models', 'entity2': '...  \n",
       "99  {'entity1': 'deep LSTM encoder', 'entity2': 'a...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "for n in range(len(test)):\n",
    "    entities.append(extract_entity(test.iloc[n, 1]))\n",
    "test['entity'] = entities\n",
    "test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>End-to-end attention-based large vocabulary sp...</td>\n",
       "      <td>{'entity1': 'Large Vocabulary Continuous Speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>{'entity1': 'deep contextualized word represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>End-to-end memory networks</td>\n",
       "      <td>{'entity1': 'neural network', 'entity2': 'recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepTox: Toxicity prediction using deep learning</td>\n",
       "      <td>{'entity1': 'Tox21 Data Challenge', 'entity2':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language models as knowledge bases?</td>\n",
       "      <td>{'entity1': 'pretraining language models', 'en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A state-of-the-art survey on deep learning the...</td>\n",
       "      <td>{'entity1': 'Deep Learning', 'entity2': 'Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>“So what if ChatGPT wrote it?” Multidisciplina...</td>\n",
       "      <td>{'entity1': 'ChatGPT', 'entity2': 'generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Sequence-level knowledge distillation</td>\n",
       "      <td>{'entity1': 'Neural machine translation (NMT)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Transformers in Vision: A Survey</td>\n",
       "      <td>{'entity1': 'Transformer models', 'entity2': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learned in translation: Contextualized word ve...</td>\n",
       "      <td>{'entity1': 'deep LSTM encoder', 'entity2': 'a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   End-to-end attention-based large vocabulary sp...   \n",
       "1            Deep contextualized word representations   \n",
       "2                          End-to-end memory networks   \n",
       "3    DeepTox: Toxicity prediction using deep learning   \n",
       "4                 Language models as knowledge bases?   \n",
       "..                                                ...   \n",
       "95  A state-of-the-art survey on deep learning the...   \n",
       "96  “So what if ChatGPT wrote it?” Multidisciplina...   \n",
       "97              Sequence-level knowledge distillation   \n",
       "98                   Transformers in Vision: A Survey   \n",
       "99  Learned in translation: Contextualized word ve...   \n",
       "\n",
       "                                               entity  \n",
       "0   {'entity1': 'Large Vocabulary Continuous Speec...  \n",
       "1   {'entity1': 'deep contextualized word represen...  \n",
       "2   {'entity1': 'neural network', 'entity2': 'recu...  \n",
       "3   {'entity1': 'Tox21 Data Challenge', 'entity2':...  \n",
       "4   {'entity1': 'pretraining language models', 'en...  \n",
       "..                                                ...  \n",
       "95  {'entity1': 'Deep Learning', 'entity2': 'Machi...  \n",
       "96  {'entity1': 'ChatGPT', 'entity2': 'generative ...  \n",
       "97  {'entity1': 'Neural machine translation (NMT)'...  \n",
       "98  {'entity1': 'Transformer models', 'entity2': '...  \n",
       "99  {'entity1': 'deep LSTM encoder', 'entity2': 'a...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[:, ['Title', 'entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End-to-end attention-based large vocabulary speech recognition'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'Large Vocabulary Continuous Speech Recognition (LVCSR) Systems',\n",
       " 'entity2': 'neural networks',\n",
       " 'entity3': 'Hidden Markov Models (HMMs)',\n",
       " 'entity4': 'Connectionist Temporal Classification modules',\n",
       " 'entity5': 'attention mechanism',\n",
       " 'entity6': 'Recurrent Neural Network (RNN)',\n",
       " 'entity7': 'n-gram language model',\n",
       " 'entity8': 'Wall Street Journal corpus'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers in Vision: A Survey'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[98, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'Transformer models',\n",
       " 'entity2': 'computer vision problems',\n",
       " 'entity3': 'Long short-term memory',\n",
       " 'entity4': 'convolutional networks',\n",
       " 'entity5': 'self-attention',\n",
       " 'entity6': 'large-scale pre-training',\n",
       " 'entity7': 'bidirectional feature encoding',\n",
       " 'entity8': 'Transformer models in computer vision discipline'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[98, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
