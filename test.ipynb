{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a highly precise information extractor designed to identify key research entities within an essay abstract.  You are very strict; you will absolutely not associate or produce information that is not in the original text. You should always give JSON format output exactly like the example output and with nothing else. Adhere strictly to the following guidelines:\\n\\n# Core Principles:\\n\\n* Focus Exclusively on the Abstract: Base your analysis solely on the provided abstract text.\\n* Identify Research Entities: Pinpoint the distinct concepts, methods, variables, or objects that are central to the research investigation.\\n* Eliminate Duplicates: Ensure each identified entity is unique within the output.\\n* Resolve Ambiguity: If an entity\\'s meaning is unclear, deduce its most likely interpretation based on the context.\\n* Simplify Complex Terminology:  If technical jargon is present, strive to express the entity in more accessible language.\\n* Highlight Novel Paradigms: If the abstract proposes a new model, framework, or approach, explicitly include it in the output.\\n* Prioritize Key Entities: Focus on extracting the most salient and relevant entities that contribute to the core research focus.\\n* Provide JSON Output Only: Present your response exclusively in the specified JSON format, with no additional commentary.\\n* Following the Chain of Thought: Following the instruction of the Chain of Thought to get the entities\\n* Acronyms: Include both the acronym and its expanded form (e.g., \"Large Language Model (LLM)\").\\n\\n# Chain of Thought\\n1. Decompose the abstract sentence by sentence base on the object or varialbe or concept of the sentence is describing, forming the information block. Identify shifts in focus or direction within the abstract using transition phrases (e.g., “however,” “therefore,” etc.), and ensure entities before and after are clearly separated.\\n2. For each information block, extract the research entities or key concept and ensure they are unique and conform to the simplification and novel paradigm principles.\\n3. Consolidate closely related entities if possible to optimize the number of entities while maintaining clarity.\\n4. Format the extracted entities into the specified JSON structure, ensuring there are no more than 10 entities in total.\\n5. Rank Entities by Importance: If feasible, rank the entities from most to least critical based on their contribution to the abstract’s core message.\\n\\n# example\\n\\n## example abstract\\n\\nLarge Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for in-formation extraction (IE) tasks, however, re-mains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. There-fore, we conclude that LLMs are not effec-tive few-shot information extractors in general 1. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our pre-liminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.\\n\\n##  example output\\n\\n{\"entity1\": \"Large Language Models (LLMs)\", \"entity2\": \"information extraction (IE)\", \"entity3\": \"fine-tuned SLMs\", \"entity4\": \"prompting strategies\", \"entity5\": \"adaptive filter-then-rerank paradigm\"}\\n\\nNow, replace the content after the colon with the unique research entities of the following essay abstract:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file='prompt.md', mode='r', encoding='utf-8') as file:\n",
    "    prompt = file.read()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import openai\n",
    "def extract_entity(abstract:str):\n",
    "\n",
    "#     openai.api_key = \"YOUR_API_KEY\" # openai version\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-40\", \n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": prompt + abstract}\n",
    "#     ],\n",
    "#     temperature=0  \n",
    "# )\n",
    "\n",
    "#     try:\n",
    "#         return json.loads(response.choices[0].message.content)\n",
    "#     except json.JSONDecodeError: \n",
    "#         return \"Not JSON format\"\n",
    "\n",
    "    response = ollama.chat(model='llama3.1', options={\"temperature\":0}, messages=[{\n",
    "        'role':'user',\n",
    "        'content': prompt + abstract\n",
    "    }])\n",
    "    \n",
    "    try: return(json.loads(response['message']['content']))\n",
    "\n",
    "    except json.JSONDecodeError: \n",
    "        return \"Not JSON format\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'decomposition-based evolutionary multi-objective optimization',\n",
       " 'entity2': 'multi-objective evolutionary algorithms based on decomposition (MOEA/D)',\n",
       " 'entity3': 'data mining approaches',\n",
       " 'entity4': 'knowledge graph',\n",
       " 'entity5': 'topic modeling techniques'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_abstract = 'Abstract:This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature re-lated to multi-objective evolutionary algorithms based on decomposition (MOEA/D). Complementary to the first part, here we employ a series of advanced data mining approaches to provide a compre-hensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol. In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5, 400 papers, 10, 000 authors, 400 venues, and 1, 600 institutions for MOEA/D research. We start our analysis with basic descriptive statistics. Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships. We also explored the col-laboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers. Our data mining results here, combined with the ex-pert review in Part I1, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective. Keywords: Multi-objective optimization, decomposition, data mining, topic modeling, network analysis, data visualization.'\n",
    "extract_entity(abstract=test_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('source_document.csv')\n",
    "test = test[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>End-to-end attention-based large vocabulary sp...</td>\n",
       "      <td>Many state-of-the-art Large Vocabulary Continu...</td>\n",
       "      <td>{'entity1': 'Large Vocabulary Continuous Speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "      <td>{'entity1': 'deep contextualized word represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>End-to-end memory networks</td>\n",
       "      <td>We introduce a neural network with a recurrent...</td>\n",
       "      <td>{'entity1': 'neural network', 'entity2': 'recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepTox: Toxicity prediction using deep learning</td>\n",
       "      <td>The Tox21 Data Challenge has been the largest ...</td>\n",
       "      <td>{'entity1': 'Tox21 Data Challenge', 'entity2':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language models as knowledge bases?</td>\n",
       "      <td>Recent progress in pretraining language models...</td>\n",
       "      <td>{'entity1': 'pretraining language models', 'en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A state-of-the-art survey on deep learning the...</td>\n",
       "      <td>In recent years, deep learning has garnered tr...</td>\n",
       "      <td>{'entity1': 'Deep Learning', 'entity2': 'Super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>“So what if ChatGPT wrote it?” Multidisciplina...</td>\n",
       "      <td>Transformative artificially intelligent tools,...</td>\n",
       "      <td>{'entity1': 'ChatGPT', 'entity2': 'computer sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Sequence-level knowledge distillation</td>\n",
       "      <td>Neural machine translation (NMT) offers a nove...</td>\n",
       "      <td>{'entity1': 'Neural machine translation (NMT)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Transformers in Vision: A Survey</td>\n",
       "      <td>Astounding results from Transformer models on ...</td>\n",
       "      <td>{'entity1': 'Transformer models', 'entity2': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learned in translation: Contextualized word ve...</td>\n",
       "      <td>Computer vision has benefited from initializin...</td>\n",
       "      <td>{'entity1': 'pretrained word vectors', 'entity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   End-to-end attention-based large vocabulary sp...   \n",
       "1            Deep contextualized word representations   \n",
       "2                          End-to-end memory networks   \n",
       "3    DeepTox: Toxicity prediction using deep learning   \n",
       "4                 Language models as knowledge bases?   \n",
       "..                                                ...   \n",
       "95  A state-of-the-art survey on deep learning the...   \n",
       "96  “So what if ChatGPT wrote it?” Multidisciplina...   \n",
       "97              Sequence-level knowledge distillation   \n",
       "98                   Transformers in Vision: A Survey   \n",
       "99  Learned in translation: Contextualized word ve...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Many state-of-the-art Large Vocabulary Continu...   \n",
       "1   We introduce a new type of deep contextualized...   \n",
       "2   We introduce a neural network with a recurrent...   \n",
       "3   The Tox21 Data Challenge has been the largest ...   \n",
       "4   Recent progress in pretraining language models...   \n",
       "..                                                ...   \n",
       "95  In recent years, deep learning has garnered tr...   \n",
       "96  Transformative artificially intelligent tools,...   \n",
       "97  Neural machine translation (NMT) offers a nove...   \n",
       "98  Astounding results from Transformer models on ...   \n",
       "99  Computer vision has benefited from initializin...   \n",
       "\n",
       "                                               entity  \n",
       "0   {'entity1': 'Large Vocabulary Continuous Speec...  \n",
       "1   {'entity1': 'deep contextualized word represen...  \n",
       "2   {'entity1': 'neural network', 'entity2': 'recu...  \n",
       "3   {'entity1': 'Tox21 Data Challenge', 'entity2':...  \n",
       "4   {'entity1': 'pretraining language models', 'en...  \n",
       "..                                                ...  \n",
       "95  {'entity1': 'Deep Learning', 'entity2': 'Super...  \n",
       "96  {'entity1': 'ChatGPT', 'entity2': 'computer sc...  \n",
       "97  {'entity1': 'Neural machine translation (NMT)'...  \n",
       "98  {'entity1': 'Transformer models', 'entity2': '...  \n",
       "99  {'entity1': 'pretrained word vectors', 'entity...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "for n in range(len(test)):\n",
    "    entities.append(extract_entity(test.iloc[n, 1]))\n",
    "test['entity'] = entities\n",
    "test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>End-to-end attention-based large vocabulary sp...</td>\n",
       "      <td>{'entity1': 'Large Vocabulary Continuous Speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>{'entity1': 'deep contextualized word represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>End-to-end memory networks</td>\n",
       "      <td>{'entity1': 'neural network', 'entity2': 'recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepTox: Toxicity prediction using deep learning</td>\n",
       "      <td>{'entity1': 'Tox21 Data Challenge', 'entity2':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language models as knowledge bases?</td>\n",
       "      <td>{'entity1': 'pretraining language models', 'en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A state-of-the-art survey on deep learning the...</td>\n",
       "      <td>{'entity1': 'Deep Learning', 'entity2': 'Super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>“So what if ChatGPT wrote it?” Multidisciplina...</td>\n",
       "      <td>{'entity1': 'ChatGPT', 'entity2': 'computer sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Sequence-level knowledge distillation</td>\n",
       "      <td>{'entity1': 'Neural machine translation (NMT)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Transformers in Vision: A Survey</td>\n",
       "      <td>{'entity1': 'Transformer models', 'entity2': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learned in translation: Contextualized word ve...</td>\n",
       "      <td>{'entity1': 'pretrained word vectors', 'entity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   End-to-end attention-based large vocabulary sp...   \n",
       "1            Deep contextualized word representations   \n",
       "2                          End-to-end memory networks   \n",
       "3    DeepTox: Toxicity prediction using deep learning   \n",
       "4                 Language models as knowledge bases?   \n",
       "..                                                ...   \n",
       "95  A state-of-the-art survey on deep learning the...   \n",
       "96  “So what if ChatGPT wrote it?” Multidisciplina...   \n",
       "97              Sequence-level knowledge distillation   \n",
       "98                   Transformers in Vision: A Survey   \n",
       "99  Learned in translation: Contextualized word ve...   \n",
       "\n",
       "                                               entity  \n",
       "0   {'entity1': 'Large Vocabulary Continuous Speec...  \n",
       "1   {'entity1': 'deep contextualized word represen...  \n",
       "2   {'entity1': 'neural network', 'entity2': 'recu...  \n",
       "3   {'entity1': 'Tox21 Data Challenge', 'entity2':...  \n",
       "4   {'entity1': 'pretraining language models', 'en...  \n",
       "..                                                ...  \n",
       "95  {'entity1': 'Deep Learning', 'entity2': 'Super...  \n",
       "96  {'entity1': 'ChatGPT', 'entity2': 'computer sc...  \n",
       "97  {'entity1': 'Neural machine translation (NMT)'...  \n",
       "98  {'entity1': 'Transformer models', 'entity2': '...  \n",
       "99  {'entity1': 'pretrained word vectors', 'entity...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[:, ['Title', 'entity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End-to-end attention-based large vocabulary speech recognition'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'Large Vocabulary Continuous Speech Recognition (LVCSR) Systems',\n",
       " 'entity2': 'neural networks',\n",
       " 'entity3': 'Hidden Markov Models (HMMs)',\n",
       " 'entity4': 'Connectionist Temporal Classification modules',\n",
       " 'entity5': 'attention mechanism',\n",
       " 'entity6': 'Recurrent Neural Network (RNN)'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers in Vision: A Survey'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[98, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity1': 'Transformer models',\n",
       " 'entity2': 'computer vision problems',\n",
       " 'entity3': 'self-attention',\n",
       " 'entity4': 'large-scale pre-training',\n",
       " 'entity5': 'bidirectional feature encoding'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[98, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9597766946729493]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_test = \"Decomposition-based evolutionary multi-objective optimization (MOEA/D) has emerged as a powerful paradigm within the realm of multi-objective evolutionary algorithms (MOEAs), demonstrating remarkable efficacy in tackling intricate optimization challenges across diverse domains. This abstract delves into the application of data mining approaches, encompassing knowledge graph construction, topic modeling techniques, and analysis of citation networks, to illuminate the evolutionary trajectory and knowledge landscape associated with MOEA/D and its pivotal concept of decomposition within the broader context of MOEAs. Through the lens of data-driven insights, we aim to unravel the historical progression, thematic clusters, and influential contributions that have shaped the development and impact of MOEA/D, ultimately fostering a deeper comprehension of its significance and potential avenues for future exploration in the dynamic field of multi-objective optimization.\"\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "reranker.compute_score([pre_test, test_abstract], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "abstracts.append('Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences that are cognitively accessible to experts dramatically improves (by up to 400%) AI prediction of future discoveries beyond models focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising ‘alien’ hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. By accelerating human discovery or probing its blind spots, human-aware AI enables us to move towards and beyond the contemporary scientific frontier.')\n",
    "abstracts.append('The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as “What are the main themes in the dataset?”, since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be in-dexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pre-generate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na¨ıve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.')\n",
    "abstracts.append('Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’  false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a  risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.')\n",
    "abstracts.append('Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment. Our code is available at https://github.com/mayubo2333/LLM-IE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity1': 'Artificial intelligence (AI) models',\n",
       "  'entity2': 'human scientists',\n",
       "  'entity3': 'unsupervised models',\n",
       "  'entity4': 'simulated inferences',\n",
       "  'entity5': 'human-aware AI'},\n",
       " {'entity1': 'retrieval-augmented generation (RAG)',\n",
       "  'entity2': 'large language models (LLMs)',\n",
       "  'entity3': 'query-focused summarization (QFS)',\n",
       "  'entity4': 'Graph RAG',\n",
       "  'entity5': 'entity knowledge graph'},\n",
       " {'entity1': 'Large language model (LLM) systems',\n",
       "  'entity2': 'ChatGPT1 or Gemini2',\n",
       "  'entity3': 'hallucinations',\n",
       "  'entity4': 'entropy-based uncertainty estimators',\n",
       "  'entity5': 'confabulations'},\n",
       " {'entity1': 'Large Language Models (LLMs)',\n",
       "  'entity2': 'information extraction (IE)',\n",
       "  'entity3': 'fine-tuned SLMs',\n",
       "  'entity4': 'prompting strategies',\n",
       "  'entity5': 'adaptive filter-then-rerank paradigm'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_entity = []\n",
    "for abs in abstracts:\n",
    "    ex_entity.append(extract_entity(abs))\n",
    "\n",
    "ex_entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
